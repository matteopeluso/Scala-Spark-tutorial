{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheat Sheet Scala-Spark environment on Jupyter-lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.SparkContext\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Creare un Dataset, fornita una sequenza in ingresso\n",
    "\n",
    "val data = Seq((1,2,3), (4,5,6), (6,7,8), (9,19,10))\n",
    "val ds   = spark.createDataset(data)\n",
    "\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:29\n",
       "filtered: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:32\n",
       "res5: Array[String] = Array(ABC, BCD)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generazione di un Resilient Distributed Dataset\n",
    "val rdd      = sc.parallelize(List(\"ABC\",\"BCD\",\"DEF\"))\n",
    "\n",
    "// filter: new RDD by selecting those data elements on which func returns true\n",
    "val filtered = rdd.filter(_.contains(\"C\"))\n",
    "// Finché non costretto da un'operazione Spark mantiene tutto in cache\n",
    "// collect(): get all the data elements in an RDD as an array\n",
    "filtered.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at <console>:29\n",
       "times2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at map at <console>:31\n",
       "res17: Array[Int] = Array(2, 4, 6, 8, 10)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generazione di un Resilient Distributed Dataset\n",
    "val rdd      = sc.parallelize(List(1,2,3,4,5))\n",
    "// map: return new RDD by applying func on each data element\n",
    "val times2   = rdd.map(_*2)\n",
    "times2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[15] at parallelize at <console>:29\n",
       "fm: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at flatMap at <console>:31\n",
       "res11: Array[String] = Array(Spark, is, awesome”,”It, fun)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generazione di un Resilient Distributed Dataset\n",
    "val rdd      = sc.parallelize(List(\"Spark is awesome”,”It is fun\"))\n",
    "// flatMap: Similar to map but func returns a Seq instead of a value. \n",
    "val fm       = rdd.flatMap(str=>str.split(\" \"))\n",
    "// distinct: Eliminate duplicates from RDD\n",
    "fm.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[20] at map at <console>:30\n",
       "wrdCnt: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:33\n",
       "cntWrd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[22] at map at <console>:36\n",
       "res12: Array[(Int, Iterable[String])] = Array((1,CompactBuffer(Spark, awesome”,”It, fun)), (2,CompactBuffer(is)))\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word1   = fm.map(word=>(word,1))\n",
    "// reduceByKey: to aggregate values of a key using a function. “numTasks” is an optional\n",
    "// parameter to specify number of reduce tasks\n",
    "val wrdCnt  = word1.reduceByKey(_+_)\n",
    "wrdCnt.collect()\n",
    "\n",
    "val cntWrd = wrdCnt.map{case (word, count) => (count, word)}\n",
    "cntWrd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[66] at parallelize at <console>:29\n",
       "rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[67] at parallelize at <console>:30\n",
       "res22: Array[(String, String)] = Array((A,B), (A,C), (B,B), (B,C))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generazione di un Resilient Distributed Dataset\n",
    "val rdd1=sc.parallelize(List(\"A\",\"B\"))\n",
    "val rdd2=sc.parallelize(List(\"B\",\"C\"))\n",
    "// union(): new RDD containing all elements from source RDD and argument.\n",
    "rdd1.union(rdd2).collect()\n",
    "// intersection(): new RDD containing all elements from source RDD and argument.\n",
    "rdd1.intersection(rdd2).collect()\n",
    "// cartesian(): new RDD cross product of all elements from source RDD and argument.\n",
    "rdd1.cartesian(rdd2).collect()\n",
    "// subtract() :new RDD created by removing data elements in source RDD in common with argument \n",
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personFruit: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[76] at parallelize at <console>:25\n",
       "personSE: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[77] at parallelize at <console>:26\n",
       "res23: Array[(String, (String, String))] = Array((Bob,(Banana,Bing)), (Bob,(Banana,AltaVista)), (Andy,(Apple,Google)), (Andy,(Apricot,Google)), (Charlie,(Cherry,Yahoo)))\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personFruit = sc.parallelize(Seq((\"Andy\", \"Apple\"), (\"Bob\", \"Banana\"), (\"Charlie\", \"Cherry\"), (\"Andy\",\"Apricot\")))\n",
    "val personSE    = sc.parallelize(Seq((\"Andy\",\"Google\"), (\"Bob\", \"Bing\"), (\"Charlie\", \"Yahoo\"), (\"Bob\",\"AltaVista\")))\n",
    "// join(RDD,[numTasks]): When invoked on(K,V) and (K,W), this operationcreates a new RDD of (K,(V,W))\n",
    "personFruit.join(personSE).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[81] at parallelize at <console>:27\n",
       "res24: Long = 3\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(\"A\",\"B\",\"C\"))\n",
    "// count(): Get the number of data elements in the RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[82] at parallelize at <console>:27\n",
       "res25: Int = 10\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,4))\n",
    "// reduce(func): Aggregate the data elements in an RDD using this function which takes two arguments and returns one\n",
    "rdd.reduce(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[83] at parallelize at <console>:27\n",
       "res26: Array[Int] = Array(1, 2)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,4))\n",
    "//take (n): fetch first n data elements in an RDD. Computed by driver program. \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1*10=10\n",
      "2*10=20\n",
      "3*10=30\n",
      "4*10=40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at parallelize at <console>:29\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(List(1,2,3,4))\n",
    "// foreach(func): execute function foreach data element in RDD. \n",
    "rdd.foreach(x=>println(\"%s*10=%s\".format(x,x*10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nErrors: org.apache.spark.Accumulator[Double] = 2.0\n",
       "logs: org.apache.spark.rdd.RDD[String] = output.log MapPartitionsRDD[1] at textFile at <console>:27\n",
       "res1: Double = 2.0\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Accumulators\n",
    "val nErrors=sc.accumulator(0.0)\n",
    "val logs = sc.textFile(\"output.log\")\n",
    "logs.filter(_.contains(\"error\")).foreach(x=>nErrors+=1)\n",
    "nErrors.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map: scala.collection.immutable.Map[String,Int] = Map(ground -> 1, med -> 2, priority -> 5, express -> 10)\n",
       "bcMailRates: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] = Broadcast(23)\n",
       "pts: org.apache.spark.rdd.RDD[String] = packagesToShip.txt MapPartitionsRDD[27] at textFile at <console>:30\n",
       "shippingCost: org.apache.spark.Accumulator[Double] = 0.0\n",
       "res5: Double = 0.0\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val map = sc.parallelize(Seq((\"ground\",1),(\"med\",2),(\"priority\",5),(\"express\",10))).collect().toMap\n",
    "val bcMailRates = sc.broadcast(map)\n",
    "// In the above command, we create a broadcast variable, a map containing cost by class of service.\n",
    "val pts = sc.textFile(\"packagesToShip.txt\")\n",
    "//pts.map(shipType=>(shipType,1)).reduceByKey(_+_).map{case (shipType,nPackages)=>(shipType,nPackages*bcMailRates.value(shipType))}.collect()\n",
    "\n",
    "val shippingCost=sc.accumulator(0.0)\n",
    "//pts.map(x=>(x,1)).reduceByKey(_+_).map{case(x,y)=>(x,y*bcMailRates.value(x))}.foreach(v=>shippingCost+=v._2)\n",
    "shippingCost.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Customer(name:String,age:Int,gender:String,address:String)\n",
    "\n",
    "spark.driver.allowMultipleContexts = true\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sparkConf  = new SparkConf().setAppName(\"Customers\")\n",
    "val sc         = new SparkContext(sparkConf)\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val r          = sc.textFile(\"customers.txt\")\n",
    "val records    = r.map(_.split(\"|\"))\n",
    "val c          = records.map(r=>Customer(r(0),r(1).trim.toInt,r(2),r(3)))\n",
    "\n",
    "\n",
    "c.registerAsTable(\"customers\")\n",
    "sqlContext.sql(\"select * from customers where gender='M' and age < 30\").collect().foreach(println) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
